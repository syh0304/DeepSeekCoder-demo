# -*- coding: utf-8 -*-
"""DeepSeek Coder 训练笔记本 - Google Colab 版本.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YOUR_DRIVE_LINK

# DeepSeek Coder 训练笔记本

这个笔记本演示了如何在 Google Colab 上训练和微调 DeepSeek Coder 模型。

## 设置环境

首先安装必要的依赖包：
"""

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install transformers accelerate datasets peft bitsandbytes

# 检查 GPU 信息
import torch
from pprint import pprint

print("GPU 信息:")
print(f"可用 GPU: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU 名称: {torch.cuda.get_device_name(0)}")
    print(f"GPU 内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

"""## 加载模型和分词器"""

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# 配置模型量化以减少内存使用
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# 选择适合 Colab 内存的模型版本
model_name = "deepseek-ai/deepseek-coder-1.3b"  # 较小的模型，适合 Colab

# 加载分词器
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 加载模型
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

print("模型加载完成!")

"""## 准备训练数据"""

from datasets import load_dataset

# 创建一个简单的代码数据集示例
sample_data = {
    "code": [
        "def fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)",
        "def factorial(n):\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n-1)",
        "def is_prime(n):\n    if n <= 1:\n        return False\n    for i in range(2, int(n**0.5)+1):\n        if n % i == 0:\n            return False\n    return True"
    ]
}

# 将数据保存为临时文件并加载为数据集
import json
import tempfile

# 创建临时文件
with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
    for item in sample_data["code"]:
        f.write(json.dumps({"text": item}) + '\n')
    temp_file_path = f.name

# 加载数据集
dataset = load_dataset('json', data_files=temp_file_path, split='train')
dataset = dataset.train_test_split(test_size=0.1)

print(f"训练集大小: {len(dataset['train'])}")
print(f"测试集大小: {len(dataset['test'])}")

"""## 数据预处理"""

def preprocess_function(examples):
    # 对代码进行分词
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=256,
        return_tensors="pt"
    )

# 对数据集进行预处理
tokenized_dataset = dataset.map(
    preprocess_function,
    batched=True,
    remove_columns=dataset["train"].column_names
)

"""## 配置训练参数"""

from transformers import TrainingArguments, Trainer

# 配置训练参数（针对 Colab 环境优化）
training_args = TrainingArguments(
    output_dir="./deepseek-coder-finetuned",
    evaluation_strategy="steps",
    eval_steps=100,
    learning_rate=2e-5,
    per_device_train_batch_size=1,  # 小批量以适应 Colab 内存
    per_device_eval_batch_size=1,
    num_train_epochs=1,
    weight_decay=0.01,
    save_steps=500,
    fp16=True,  # 使用混合精度训练
    logging_steps=50,
    push_to_hub=False,
    report_to="none",
    gradient_accumulation_steps=4,  # 梯度累积
)

"""## 创建 Trainer 并开始训练"""

from transformers import DataCollatorForLanguageModeling

# 创建数据收集器
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # 使用因果语言建模而不是掩码语言建模
)

# 创建 Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=data_collator,
)

# 开始训练
print("开始训练...")
trainer.train()

# 保存模型
trainer.save_model()
tokenizer.save_pretrained("./deepseek-coder-finetuned")

print("训练完成! 模型已保存到 './deepseek-coder-finetuned'")

"""## 测试训练后的模型"""

# 加载微调后的模型
from transformers import pipeline

# 创建代码生成管道
code_generator = pipeline(
    "text-generation",
    model="./deepseek-coder-finetuned",
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1,
)

# 测试模型
test_prompts = [
    "编写一个Python函数计算斐波那契数列",
    "实现一个快速排序算法",
    "创建一个简单的HTTP服务器"
]

for prompt in test_prompts:
    print(f"\n提示: {prompt}")
    result = code_generator(
        prompt,
        max_length=200,
        temperature=0.7,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    print(f"生成的代码:\n{result[0]['generated_text']}")
    print("-" * 50)

"""## 模型评估"""

import math

# 评估模型
eval_results = trainer.evaluate()
print(f"评估结果: {eval_results}")

# 计算困惑度
if 'eval_loss' in eval_results:
    perplexity = math.exp(eval_results['eval_loss'])
    print(f"困惑度: {perplexity:.2f}")

"""## 保存到 Google Drive（可选）"""

from google.colab import drive

# 挂载 Google Drive
drive.mount('/content/drive')

# 将模型保存到 Google Drive
!cp -r ./deepseek-coder-finetuned "/content/drive/MyDrive/deepseek-coder-finetuned"

print("模型已保存到 Google Drive!")

"""## 清理临时文件"""

import os

# 清理临时文件
if os.path.exists(temp_file_path):
    os.remove(temp_file_path)

print("临时文件已清理")

"""## 使用说明

1. 在 Google Colab 中打开这个笔记本
2. 依次运行每个代码单元格
3. 根据需要修改模型名称、训练参数和数据
4. 训练完成后，模型会自动保存到 Colab 环境和 Google Drive

注意：由于 Colab 的内存限制，建议使用较小的模型（如 1.3B 版本）进行微调。

## 故障排除

如果遇到内存不足的问题，可以尝试：
1. 减少批量大小
2. 增加梯度累积步数
3. 使用更小的模型
4. 减少序列最大长度
"""

# 显示当前内存使用情况
!nvidia-smi
